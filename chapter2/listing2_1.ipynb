{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation: (array([-0.4490828,  0.       ], dtype=float32), {})\n",
      "possible actions: 3\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.449637   -0.00055423]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.4517414 -0.0021044]\n",
      "\n",
      "taking action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/apress/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got reward: -1.0. New state/observation is: [-0.45538056 -0.00363917]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.4585278  -0.00314724]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.46315998 -0.00463218]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.467243   -0.00408299]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.4707466  -0.00350365]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.473645   -0.00289838]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.47591662 -0.00227163]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.47754467 -0.00162803]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.478517   -0.00097234]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.4808264  -0.00230942]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.48345575 -0.00262933]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.48538542 -0.00192968]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.48660108 -0.00121565]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.48709363 -0.00049256]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.48885944 -0.00176581]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.49188533 -0.00302588]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.4951487  -0.00326337]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.49762517 -0.00247649]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5012963  -0.00367109]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5051345  -0.00383824]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5081112  -0.00297665]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.51220393 -0.00409276]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.51638216 -0.00417821]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.51961446 -0.00323233]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.52387667 -0.00426221]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5291368  -0.00526013]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5333554  -0.00421859]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5365008  -0.00314543]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5405495  -0.00404869]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.54447114 -0.00392161]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5472363  -0.00276517]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5488243  -0.00158804]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-5.4922336e-01 -3.9902769e-04]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5504304  -0.00120703]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-5.5043644e-01 -6.0125931e-06]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.55124134 -0.00080495]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.55183923 -0.00059787]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5532255  -0.00138632]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.55538994 -0.00216441]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5573163  -0.00192634]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5579902  -0.00067388]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5594066  -0.00141641]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5615549  -0.00214836]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.56241924 -0.00086431]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.56299305 -0.00057381]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5622721   0.00072096]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5612617   0.00101036]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-5.6096953e-01  2.9222877e-04]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.56039757  0.00057192]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5595502   0.00084735]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5584338   0.00111647]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-5.5805653e-01  3.7725407e-04]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.55742127  0.00063523]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-5.5753285e-01 -1.1153909e-04]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5563903   0.00114253]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5540022   0.00238807]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5513865   0.00261578]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5495625   0.00182394]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.54754406  0.00201847]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.54534614  0.00219791]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5419852  0.0033609]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5374865   0.00449873]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.53388363  0.00360286]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5302037   0.00367998]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.52547413  0.00472951]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5217306   0.00374358]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.519001    0.00272957]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5163059   0.00269508]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5126655   0.00364039]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5091071   0.00355841]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5056574   0.00344976]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5013421   0.00431526]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.49819365  0.00314846]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.49623555  0.0019581 ]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.49448243  0.00175311]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.4919474   0.00253502]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.49064943  0.00129799]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.48859817  0.00205127]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.4878089   0.00078925]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-4.8828757e-01 -4.7866159e-04]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-4.8803058e-01  2.5699948e-04]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.48703983  0.00099074]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.48532274  0.0017171 ]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.48389208  0.00143066]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.4817585   0.00213357]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.48093793  0.00082059]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.4804364  0.0005015]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.47925773  0.00117869]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-4.7941062e-01 -1.5288903e-04]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.47889397  0.00051667]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-4.7871158e-01  1.8238765e-04]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-4.7886482e-01 -1.5324990e-04]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.47835258  0.00051225]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.4791786  -0.00082605]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-4.7933683e-01 -1.5821998e-04]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.48082605 -0.00148921]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.48163518 -0.00080913]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-4.8175821e-01 -1.2302207e-04]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-4.8219419e-01 -4.3600306e-04]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.48293993 -0.00074574]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.48498985 -0.00204993]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.4863287  -0.00133885]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.4869465  -0.00061779]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.48783863 -0.00089213]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.48899844 -0.00115981]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.4914173  -0.00241885]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.49407712 -0.00265984]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.4959581  -0.00188096]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.49904612 -0.00308803]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5013181  -0.00227201]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.50275713 -0.00143899]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.50435233 -0.0015952 ]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5050918  -0.00073947]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.50596994 -0.0008782 ]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-5.0598031e-01 -1.0351728e-05]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5051227   0.00085757]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.50340366  0.00171907]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.50083596  0.0025677 ]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.49843886  0.00239711]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.49723026  0.00120859]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.49521923  0.00201104]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.49342078  0.00179845]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.49184838  0.00157242]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-4.9151373e-01  3.3465168e-04]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-4.9141932e-01  9.4385119e-05]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.49256593 -0.00114659]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.4939449 -0.001379 ]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.495546   -0.00160111]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.49835727 -0.00281125]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.50235766 -0.00400038]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.50651723 -0.00415959]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5118049  -0.00528764]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.51818097 -0.00637608]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.52359766 -0.00541671]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5300144  -0.00641672]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.535383  -0.0053686]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.54066324 -0.00528024]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5458155  -0.00515231]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5518013  -0.00598581]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5565759  -0.00477454]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.56010354 -0.00352762]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5643579  -0.00425438]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5693073  -0.00494945]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.57491505 -0.00560771]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5811394  -0.00622435]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.58593434 -0.00479494]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5902645  -0.00433014]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5930979  -0.00283348]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.59441394 -0.001316  ]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5952028  -0.00078887]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-5.9545881e-01 -2.5596222e-04]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.59617996 -0.00072118]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-5.9636104e-01 -1.8110956e-04]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5950008   0.00136028]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5931091   0.00189171]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5906998   0.00240927]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.58779067  0.00290914]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5854031  0.0023876]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5815546   0.00384849]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5772736   0.00428097]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5715918   0.00568179]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5645513  0.0070405]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.55620444  0.00834687]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.54761344  0.00859102]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.53784245  0.00977098]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.52796465  0.00987777]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.51705414  0.01091051]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.50719273  0.00986143]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.4964543   0.01073844]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.48691922  0.00953508]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.47665867  0.01026054]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.46574903  0.01090965]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.45427108  0.01147795]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.44230935  0.01196173]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.42995125  0.01235809]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.41728634  0.01266493]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.40540528  0.01188104]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.39339224  0.01201305]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.3813311   0.01206114]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.37030485  0.01102624]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.35938823  0.01091663]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.34865397  0.01073427]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.34017238  0.00848159]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.3329981   0.00717428]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.32617667  0.00682141]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.31975088  0.0064258 ]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.3157604   0.00399047]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.31322968  0.00253074]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-3.1317404e-01  5.5641965e-05]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.31359383 -0.00041979]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.3144865  -0.00089268]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.3178467  -0.00336016]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.32365385 -0.00580716]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.32987225 -0.00621843]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.3364632  -0.00659095]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.3433851 -0.0069219]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.35259372 -0.00920861]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.36302933 -0.01043563]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.3746232  -0.01159386]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.38829756 -0.01367435]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.40295902 -0.01466147]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.41750568 -0.01454665]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.43183467 -0.01432898]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.4468432  -0.01500855]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.46142235 -0.01457914]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.47646514 -0.01504277]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.49086022 -0.01439509]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.50550044 -0.01464024]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.52127635 -0.01577591]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.5360697  -0.01479333]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5517695  -0.01569982]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5672583  -0.01548879]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.5824206  -0.01516228]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.598144  -0.0157234]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.6133129  -0.01516896]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.62781715 -0.0145042 ]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.6415523  -0.01373523]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.65542126 -0.01386893]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.66932714 -0.01390583]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.68317443 -0.01384735]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.6948702  -0.01169573]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.7043372  -0.00946697]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.71251404 -0.00817688]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.7193487  -0.00683466]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.7247982  -0.00544948]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.72982866 -0.00503045]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.73440915 -0.00458053]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.73751193 -0.00310276]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.7391182  -0.00160629]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.7382184   0.00089982]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.73581785  0.00240053]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.73093104  0.00488681]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.7235876   0.00734346]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.7158326   0.00775499]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.70671445  0.00911815]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.69729096  0.00942348]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.68762296  0.00966801]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.67777383  0.00984915]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.6678092   0.00996467]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.65579635  0.01201282]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.6418178   0.01397851]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.62597114  0.01584669]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.6083687   0.01760247]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.58913726  0.01923141]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.57041746  0.01871979]\n",
      "\n",
      "taking action: 0\n",
      "got reward: -1.0. New state/observation is: [-0.5523477   0.01806977]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.53406256  0.01828512]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.51569897  0.01836359]\n",
      "\n",
      "taking action: 2\n",
      "got reward: -1.0. New state/observation is: [-0.49639463  0.01930435]\n",
      "\n",
      "taking action: 1\n",
      "got reward: -1.0. New state/observation is: [-0.4772941   0.01910054]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# to render environment for visual inspection\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# when you train, you can skip rendering to speed up\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done: \n\u001b[0;32m---> 28\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# based on curret policy, use the current observation\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# to find the best action to take.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     action \u001b[38;5;241m=\u001b[39m policy(obs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/apress/lib/python3.8/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/apress/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/apress/lib/python3.8/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/apress/lib/python3.8/site-packages/gym/envs/classic_control/mountain_car.py:264\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "# reset environment and get initial observation/state\n",
    "# Observation/state is a tuple of (position, velocity)\n",
    "obs = env.reset()\n",
    "print(\"initial observation:\", obs)\n",
    "\n",
    "# possible 3 actions\n",
    "# {0: \"accelerate to left\", \"1\": \"do nothing\", \"2\": \"accelerate to right\"}\n",
    "print(\"possible actions:\", env.action_space.n)\n",
    "\n",
    "\n",
    "# reinforcement learning is all\n",
    "# about learing to take good actions\n",
    "# from a given state/observation\n",
    "# right now taking a random action\n",
    "def policy(observation):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "\n",
    "# take 5 random actions/steps\n",
    "for _ in range(5):\n",
    "\n",
    "    done = False\n",
    "\n",
    "    # to render environment for visual inspection\n",
    "    # when you train, you can skip rendering to speed up\n",
    "    while not done: \n",
    "        env.render()\n",
    "\n",
    "        # based on curret policy, use the current observation\n",
    "        # to find the best action to take.\n",
    "        action = policy(obs)\n",
    "        print(\"\\ntaking action:\", action)\n",
    "\n",
    "        # pass the action to env which will return back\n",
    "        # with new state/\"observation\" and \"reward\"\n",
    "        # there is a \"done\" flag which is true when game ends\n",
    "        # \"info\" provides some diagnostic information\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        print(\"got reward: {0}. New state/observation is: {1}\".format(reward, obs))\n",
    "\n",
    "# close the enviroment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation: (array([ 0.02303374, -0.02890669, -0.00577623, -0.01573166], dtype=float32), {})\n",
      "possible actions: 2\n",
      "\n",
      "taking action: 0\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.0224556  -0.22394533 -0.00609087  0.2751232 ]\n",
      "\n",
      "taking action: 1\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.0179767  -0.02873701 -0.0005884  -0.01947454]\n",
      "\n",
      "taking action: 1\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.01740196  0.16639338 -0.00097789 -0.31234306]\n",
      "\n",
      "taking action: 0\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.02072982 -0.02871463 -0.00722476 -0.01996869]\n",
      "\n",
      "taking action: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/apress/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.02015553  0.16651018 -0.00762413 -0.31492233]\n",
      "\n",
      "taking action: 1\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.02348573  0.3617399  -0.01392258 -0.6099999 ]\n",
      "\n",
      "taking action: 1\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.03072053  0.5570537  -0.02612257 -0.90703523]\n",
      "\n",
      "taking action: 0\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.0418616   0.36229494 -0.04426328 -0.62267596]\n",
      "\n",
      "taking action: 1\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.0491075  0.5580061 -0.0567168 -0.9289643]\n",
      "\n",
      "taking action: 1\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.06026763  0.75384593 -0.07529608 -1.2389175 ]\n",
      "\n",
      "taking action: 0\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.07534455  0.5597675  -0.10007443 -0.9707408 ]\n",
      "\n",
      "taking action: 1\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.08653989  0.75607985 -0.11948925 -1.2931088 ]\n",
      "\n",
      "taking action: 0\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.10166149  0.56266195 -0.14535142 -1.040097  ]\n",
      "\n",
      "taking action: 0\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.11291473  0.36973855 -0.16615337 -0.7963465 ]\n",
      "\n",
      "taking action: 1\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.1203095  0.5667032 -0.1820803 -1.1363493]\n",
      "\n",
      "taking action: 0\n",
      "False\n",
      "got reward: 1.0. New state/observation is: [ 0.13164356  0.37436855 -0.20480728 -0.90585285]\n",
      "\n",
      "taking action: 1\n",
      "True\n",
      "got reward: 1.0. New state/observation is: [ 0.13913094  0.5715866  -0.22292434 -1.2552904 ]\n",
      "\n",
      "taking action: 1\n",
      "True\n",
      "got reward: 0.0. New state/observation is: [ 0.15056267  0.76875573 -0.24803016 -1.608724  ]\n",
      "\n",
      "taking action: 0\n",
      "True\n",
      "got reward: 0.0. New state/observation is: [ 0.16593778  0.57727134 -0.28020462 -1.4024626 ]\n",
      "\n",
      "taking action: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/apress/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "got reward: 0.0. New state/observation is: [ 0.17748322  0.77457    -0.30825388 -1.7681746 ]\n",
      "\n",
      "taking action: 1\n",
      "True\n",
      "got reward: 0.0. New state/observation is: [ 0.19297461  0.9715856  -0.34361738 -2.1389668 ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "obs = env.reset()\n",
    "print(\"initial observation:\", obs)\n",
    "print(\"possible actions:\", env.action_space.n)\n",
    "\n",
    "\n",
    "def policy(observation):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        action = policy(obs)\n",
    "        print(\"\\ntaking action:\", action)\n",
    "\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        print(done)\n",
    "        print(\"got reward: {0}. New state/observation is: {1}\".format(reward, obs))\n",
    "\n",
    "\n",
    "# close the enviroment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
